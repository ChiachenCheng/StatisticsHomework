{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week9 Multicollinearity-Ridge Regression \n",
    "## 背景描述\n",
    "\n",
    "由于多重共线性的问题本质上在于 $|X^{'}X|\\approx 0$, 因此岭回归的本质就是在这个矩阵上做了手脚, 使得多重共线性的问题得到一定的缓解.  \n",
    "这里我们建立一元线性回归模型: $y=8+3*x_1+4*x_2+5*x_3+1*x_4+\\epsilon$, 我们人工给定 10 个值和 10 个满足正态分布的 $\\epsilon$, 使得其中 $x_3, x_4$ 具有强线性相关性.  \n",
    "由此我们构造了 10 个观测的 4 个变量，具体请见下表：\n",
    "\n",
    "## 数据描述\n",
    "\n",
    "|    var     |         1         |         2          |         3          |         4          |         5         |         6         |         7          |         8          |         9         |        10         |\n",
    "| :--------: | :---------------: | :----------------: | :----------------: | :----------------: | :---------------: | :---------------: | :----------------: | :----------------: | :---------------: | :---------------: |\n",
    "|     X1     | 5.053499791495376 | 4.896359188149629  | 4.5846019652366765 | 3.4627703344989245 | 3.270451004372899 | 4.493105477608886 | 4.4464566375800425 | 4.798063216845086  | 6.427862171550048 | 4.303787732779672 |\n",
    "|     X2     | 5.072933009683874 | 3.6512177680206648 | 5.577820866295013  | 4.068646005580284  | 5.59613146647269  | 6.323457817901293 | 6.079773035399383  | 2.9443980328330173 | 4.457913906178166 | 5.357206466723423 |\n",
    "|     X3     |        1.1        |        1.4         |        1.7         |        1.7         |        1.8        |        1.8        |        1.9         |         2          |        2.3        |        2.4        |\n",
    "|     X4     |        2.2        |         3          |        3.6         |        3.4         |        3.8        |        3.6        |        3.6         |        4.2         |        4.8        |         5         |\n",
    "| $\\epsilon$ |    0.05290865     |    -0.34306401     |     -0.5598349     |     0.64584319     |    -0.37267585    |    -0.87090483    |     0.99051878     |    -1.38493147     |    0.06964761     |     0.0842377     |\n",
    "|     Y      |    51.20514006    |    46.95088463     |    55.60525446     |    47.20873822     |    52.62320303    |    58.50224287    |    59.74898083     |    46.98685031     |    61.48488975    |    59.42442677    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数设置如下:   \n",
    "\n",
    "1. 样本量 n = 10\n",
    "2. 变量个数 p = 4\n",
    "3. 自变量 $x_1$ 的波动 $\\sigma_{x_1}=1$\n",
    "4. 误差的波动 $\\sigma_y=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题 \n",
    "1. 判断所给数据是否具有多重共线性.\n",
    "2. 若具有多重共线性, 选择适当的岭参数.\n",
    "3. 进行岭回归分析.\n",
    "\n",
    "## 解决方案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1：**  \n",
    "\n",
    "多重共线性是指自变量 x1, x2, ..., xp 之间不完全线性相关但是相关性很高的情况。此时，虽然可以得到最小二乘估计，但是精度很低。随着自变量之间相关性增加，最小二乘估计结果的方差会增大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**仿真实验**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         5.05349979 5.07293301 1.1        2.2       ]\n",
      " [1.         4.89635919 3.65121777 1.4        3.        ]\n",
      " [1.         4.58460197 5.57782087 1.7        3.6       ]\n",
      " [1.         3.46277033 4.06864601 1.7        3.4       ]\n",
      " [1.         3.270451   5.59613147 1.8        3.8       ]\n",
      " [1.         4.49310548 6.32345782 1.8        3.6       ]\n",
      " [1.         4.44645664 6.07977304 1.9        3.6       ]\n",
      " [1.         4.79806322 2.94439803 2.         4.2       ]\n",
      " [1.         6.42786217 4.45791391 2.3        4.8       ]\n",
      " [1.         4.30378773 5.35720647 2.4        5.        ]]\n",
      "     0         1         2    3    4          Y\n",
      "0  1.0  5.053500  5.072933  1.1  2.2  51.205140\n",
      "1  1.0  4.896359  3.651218  1.4  3.0  46.950885\n",
      "2  1.0  4.584602  5.577821  1.7  3.6  55.605254\n",
      "3  1.0  3.462770  4.068646  1.7  3.4  47.208738\n",
      "4  1.0  3.270451  5.596131  1.8  3.8  52.623203\n",
      "5  1.0  4.493105  6.323458  1.8  3.6  58.502243\n",
      "6  1.0  4.446457  6.079773  1.9  3.6  59.748981\n",
      "7  1.0  4.798063  2.944398  2.0  4.2  46.986850\n",
      "8  1.0  6.427862  4.457914  2.3  4.8  61.484890\n",
      "9  1.0  4.303788  5.357206  2.4  5.0  59.424427\n"
     ]
    }
   ],
   "source": [
    "# Import standard packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import additional packages\n",
    "from itertools import combinations\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn import linear_model  # 进行岭回归分析\n",
    "from ipywidgets import interact  # 互动功能\n",
    "%matplotlib inline \n",
    "\n",
    "p = 4\n",
    "n = 10\n",
    "\n",
    "# 上帝视角下的beta: 截距项为 10，三个变量前的系数分别为 2，3，4\n",
    "beta = [8]\n",
    "for i in range(p):\n",
    "    beta.append((i+2)%5+1)\n",
    "beta = np.array(beta)\n",
    "# print(beta)\n",
    "\n",
    "# 构造 X 矩阵\n",
    "X = [[5.053499791495376, 4.896359188149629, 4.5846019652366765, 3.4627703344989245, 3.270451004372899, 4.493105477608886, 4.4464566375800425, 4.798063216845086, 6.427862171550048, 4.303787732779672]]\n",
    "# X = np.random.normal(loc=1.5 , scale=0.2, size=(1,n)).tolist()\n",
    "X.append([5.072933009683874, 3.6512177680206648, 5.577820866295013, 4.068646005580284, 5.59613146647269, 6.323457817901293, 6.079773035399383, 2.9443980328330173, 4.457913906178166, 5.357206466723423])\n",
    "X.append([1.1, 1.4, 1.7, 1.7, 1.8, 1.8, 1.9, 2, 2.3, 2.4])\n",
    "X.append([2.2, 3, 3.6, 3.4, 3.8, 3.6, 3.6, 4.2, 4.8, 5])\n",
    "X.insert(0, np.ones(n))\n",
    "X = np.array(X)\n",
    "X = X.T\n",
    "\n",
    "print(X)\n",
    "\n",
    "# 生成 10 个满足正态分布的 epsilon 值\n",
    "epsilon = [0.05290865, -0.34306401, -0.5598349, 0.64584319, -0.37267585, -0.87090483, 0.99051878, -1.38493147, 0.06964761, 0.0842377]\n",
    "# epsilon = np.random.normal(loc =0.0 , scale= 1, size = (1,n))\n",
    "# print(epsilon)\n",
    "\n",
    "# 上帝视角下的Y\n",
    "Y = X @ beta + epsilon\n",
    "# Y = X @ beta + epsilon[0] # 由于 np.random.normal 生成的列表中的元素仍是一个列表\n",
    "Y = Y.T\n",
    "# print(Y)\n",
    "\n",
    "df = pd.DataFrame(X)\n",
    "df['Y'] = Y\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**对原始数据进行多元线性回归分析**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/stats/stats.py:1604: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=10\n",
      "  \"anyway, n=%i\" % int(n))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.991</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.984</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   136.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 25 Apr 2021</td> <th>  Prob (F-statistic):</th> <td>2.71e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:55:57</td>     <th>  Log-Likelihood:    </th> <td> -7.6024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    10</td>      <th>  AIC:               </th> <td>   25.20</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>     5</td>      <th>  BIC:               </th> <td>   26.72</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    7.6965</td> <td>    2.132</td> <td>    3.610</td> <td> 0.015</td> <td>    2.215</td> <td>   13.178</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    2.9804</td> <td>    0.291</td> <td>   10.242</td> <td> 0.000</td> <td>    2.232</td> <td>    3.728</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    3.9880</td> <td>    0.256</td> <td>   15.574</td> <td> 0.000</td> <td>    3.330</td> <td>    4.646</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   11.7516</td> <td>    4.308</td> <td>    2.728</td> <td> 0.041</td> <td>    0.679</td> <td>   22.825</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   -2.2089</td> <td>    2.025</td> <td>   -1.091</td> <td> 0.325</td> <td>   -7.414</td> <td>    2.996</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 4.689</td> <th>  Durbin-Watson:     </th> <td>   2.417</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.096</td> <th>  Jarque-Bera (JB):  </th> <td>   2.404</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-1.199</td> <th>  Prob(JB):          </th> <td>   0.301</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.864</td> <th>  Cond. No.          </th> <td>    164.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.991\n",
       "Model:                            OLS   Adj. R-squared:                  0.984\n",
       "Method:                 Least Squares   F-statistic:                     136.9\n",
       "Date:                Sun, 25 Apr 2021   Prob (F-statistic):           2.71e-05\n",
       "Time:                        21:55:57   Log-Likelihood:                -7.6024\n",
       "No. Observations:                  10   AIC:                             25.20\n",
       "Df Residuals:                       5   BIC:                             26.72\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          7.6965      2.132      3.610      0.015       2.215      13.178\n",
       "x1             2.9804      0.291     10.242      0.000       2.232       3.728\n",
       "x2             3.9880      0.256     15.574      0.000       3.330       4.646\n",
       "x3            11.7516      4.308      2.728      0.041       0.679      22.825\n",
       "x4            -2.2089      2.025     -1.091      0.325      -7.414       2.996\n",
       "==============================================================================\n",
       "Omnibus:                        4.689   Durbin-Watson:                   2.417\n",
       "Prob(Omnibus):                  0.096   Jarque-Bera (JB):                2.404\n",
       "Skew:                          -1.199   Prob(JB):                        0.301\n",
       "Kurtosis:                       2.864   Cond. No.                         164.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model= sm.OLS(Y, X).fit()\n",
    "Y_hat = model.fittedvalues\n",
    "beta_hat = model.params\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**对数据标准化:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0         1         2         3         4         Y\n",
      "0  1.0  0.183312  0.048525 -0.615880 -0.619712 -0.160929\n",
      "1  1.0  0.123276 -0.382698 -0.355649 -0.293548 -0.408185\n",
      "2  1.0  0.004167  0.201663 -0.095418 -0.048925  0.094805\n",
      "3  1.0 -0.424436 -0.256087 -0.095418 -0.130466 -0.393199\n",
      "4  1.0 -0.497913  0.207217 -0.008674  0.032616 -0.078512\n"
     ]
    }
   ],
   "source": [
    "# 对数据进行标准化\n",
    "# 自变量 X 的均值\n",
    "X_mean = []\n",
    "for i in range(p):\n",
    "    X_mean.append(np.mean(X[:, i+1])) \n",
    "\n",
    "# 自变量 X 的标准差\n",
    "X_L = []\n",
    "for i in range(p):\n",
    "    X_L.append(sum((X[:, i+1] - X_mean[i]) ** 2))  \n",
    "\n",
    "# 对自变量 X 标准化(截距项不用标准化)\n",
    "X_std = X * 1.0\n",
    "X_std[:,1:p+1] = (X[:,1:p+1] - X_mean) / np.sqrt(X_L)\n",
    "\n",
    "# 对因变量 Y 标准化\n",
    "Y_L = sum((Y - np.mean(Y))**2)\n",
    "Y_std = (Y - np.mean(Y)) / np.sqrt(Y_L)\n",
    "\n",
    "df_std = pd.DataFrame(X_std)\n",
    "df_std['Y'] = Y_std\n",
    "print(df_std.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**对标准化后的数据进行多元线性回归分析**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/stats/stats.py:1604: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=10\n",
      "  \"anyway, n=%i\" % int(n))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.991</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.984</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   136.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 25 Apr 2021</td> <th>  Prob (F-statistic):</th> <td>2.71e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:55:58</td>     <th>  Log-Likelihood:    </th> <td>  20.850</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    10</td>      <th>  AIC:               </th> <td>  -31.70</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>     5</td>      <th>  BIC:               </th> <td>  -30.19</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 3.053e-16</td> <td>    0.013</td> <td> 2.27e-14</td> <td> 1.000</td> <td>   -0.035</td> <td>    0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.4534</td> <td>    0.044</td> <td>   10.242</td> <td> 0.000</td> <td>    0.340</td> <td>    0.567</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.7642</td> <td>    0.049</td> <td>   15.574</td> <td> 0.000</td> <td>    0.638</td> <td>    0.890</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.7874</td> <td>    0.289</td> <td>    2.728</td> <td> 0.041</td> <td>    0.045</td> <td>    1.529</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   -0.3149</td> <td>    0.289</td> <td>   -1.091</td> <td> 0.325</td> <td>   -1.057</td> <td>    0.427</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 4.689</td> <th>  Durbin-Watson:     </th> <td>   2.417</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.096</td> <th>  Jarque-Bera (JB):  </th> <td>   2.404</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-1.199</td> <th>  Prob(JB):          </th> <td>   0.301</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.864</td> <th>  Cond. No.          </th> <td>    30.3</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.991\n",
       "Model:                            OLS   Adj. R-squared:                  0.984\n",
       "Method:                 Least Squares   F-statistic:                     136.9\n",
       "Date:                Sun, 25 Apr 2021   Prob (F-statistic):           2.71e-05\n",
       "Time:                        21:55:58   Log-Likelihood:                 20.850\n",
       "No. Observations:                  10   AIC:                            -31.70\n",
       "Df Residuals:                       5   BIC:                            -30.19\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       3.053e-16      0.013   2.27e-14      1.000      -0.035       0.035\n",
       "x1             0.4534      0.044     10.242      0.000       0.340       0.567\n",
       "x2             0.7642      0.049     15.574      0.000       0.638       0.890\n",
       "x3             0.7874      0.289      2.728      0.041       0.045       1.529\n",
       "x4            -0.3149      0.289     -1.091      0.325      -1.057       0.427\n",
       "==============================================================================\n",
       "Omnibus:                        4.689   Durbin-Watson:                   2.417\n",
       "Prob(Omnibus):                  0.096   Jarque-Bera (JB):                2.404\n",
       "Skew:                          -1.199   Prob(JB):                        0.301\n",
       "Kurtosis:                       2.864   Cond. No.                         30.3\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do the multiple linear regression——对标准化后的数据\n",
    "model_std = sm.OLS(Y_std, X_std).fit()\n",
    "beta_std_hat = model_std.params\n",
    "Y_std_hat = model_std.fittedvalues\n",
    "model_std.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**求 $(X^*)^{'}(X^*)$ 矩阵的特征值和特征向量:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.225151</td>\n",
       "      <td>0.146603</td>\n",
       "      <td>0.167910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.225151</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.045426</td>\n",
       "      <td>-0.032975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.146603</td>\n",
       "      <td>0.045426</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.167910</td>\n",
       "      <td>-0.032975</td>\n",
       "      <td>0.985999</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4\n",
       "1  1.000000 -0.225151  0.146603  0.167910\n",
       "2 -0.225151  1.000000  0.045426 -0.032975\n",
       "3  0.146603  0.045426  1.000000  0.985999\n",
       "4  0.167910 -0.032975  0.985999  1.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (X*)'(X*) 矩阵等价于原始矩阵 X 样本相关矩阵\n",
    "R = df.corr()\n",
    "R = R.iloc[1:-1,1:-1]\n",
    "\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(R, cmap='Blues', annot=True) # annot: 在heatmap中每个方格写入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征值:  [2.03537021 1.20052604 0.7532161  0.01088765]\n"
     ]
    }
   ],
   "source": [
    "# 求特征值 & 特征向量\n",
    "W, V = np.linalg.eig(R)\n",
    "W_diag = np.diag(W)\n",
    "V = V.T # 这里需要转置\n",
    "print('特征值: ', W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**判断 X 矩阵是否具有多重共线性:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIF方法判断结果(阈值为 5): \n",
      "设计矩阵 X 存在多重共线性.\n",
      "\n",
      "特征值判定法判断结果(阈值为 10): \n",
      "设计矩阵 X 存在多重共线性，其中kappa值为：13.6727\n"
     ]
    }
   ],
   "source": [
    "# 定义\"判断多重共线性\"的函数\n",
    "# 参数: (X_list: 设计矩阵 X, thres_vif: VIF 方法判断多重共线性的阈值, thres_kappa: 特征值方法判断多重共线性的阈值)\n",
    "def judge_col(X_list, thres_vif, thres_kappa): \n",
    "    var_num = X_list.shape[1]\n",
    "    print('VIF方法判断结果(阈值为 %d): '% thres_vif)\n",
    "    vif = [variance_inflation_factor(X_list, i) for i in range(var_num)]\n",
    "    for i in range(var_num):\n",
    "        if vif[i] >= thres_vif:\n",
    "            print('设计矩阵 X 存在多重共线性.')\n",
    "            break\n",
    "        elif i == var_num-1:\n",
    "            print('设计矩阵 X 不存在多重共线性.')\n",
    "\n",
    "    print('\\n特征值判定法判断结果(阈值为 %d): '% thres_kappa)\n",
    "    kappa = []\n",
    "    for i in range(var_num):\n",
    "        kappa.append(np.sqrt(max(W) / W[i]))\n",
    "    if np.max(kappa) >= thres_kappa:\n",
    "        print('设计矩阵 X 存在多重共线性，其中kappa值为：%.4f'% np.max(kappa))\n",
    "    else:\n",
    "        print('设计矩阵 X 不存在多重共线性，其中kappa值为：%.4f'% np.max(kappa))\n",
    "\n",
    "# 判断多重共线性\n",
    "X_std1 = X_std[:,1:p+1] # 将 X 矩阵的截距项去掉\n",
    "beta_std_hat1 = beta_std_hat[1:p+1] # 将 β_0 去掉 \n",
    "judge_col(X_std1, 5, 10) # 判断多重共线性\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2:**  \n",
    "\n",
    "称$\\hat\\beta(k)=(\\pmb{X}'\\pmb{X}+k\\pmb{I})^{-1}\\pmb{X}'\\pmb{y}$为岭回归估计，其中k为岭参数。这是为了求解逆矩阵更加方便，且在k很小时与最小二乘估计大约相等。\n",
    "\n",
    "岭参数 $k$ 选择(模型选择)的方法:   \n",
    "1. 岭迹法\n",
    "2. 方差扩大因子法\n",
    "3. 霍尔-肯纳德（Hoerl-Kennad）公式\n",
    "4. Mcdorard-Garaneau 公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 岭迹法  \n",
    "岭迹法的一般原则:   \n",
    "· 系数岭估计基本稳定;    \n",
    "· 最小二乘回归下符号不合理的回归系数, 在岭估计的意义下符号变得合理;    \n",
    "· 回归系数合乎经济意义;  \n",
    "· 残差平方和不会增大太多."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a0685ab06374daa8cf612a1cde7bb6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='K', min=1), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.RR1(K=1)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [0, 1) 范围内划分的最大精细程度: [0,0.01,0.02,...,0.99]\n",
    "range_const1 = 100\n",
    "\n",
    "def RR1(K = 1):\n",
    "    # 计算岭估计\n",
    "    rang1 = []\n",
    "    for i in range(K):\n",
    "        rang1.append(i/K) # 岭参数 k 取值范围: [0,1-1/K](例如 K=10, k 取值范围: [0, 0.9])\n",
    "    coefs_1 = []\n",
    "    for k in rang1:\n",
    "        temp1 = np.linalg.inv(X_std1.T @ X_std1 + k * np.eye(p)) @ X_std1.T @ Y_std\n",
    "        coefs_1.append(temp1)\n",
    "\n",
    "    # 画图\n",
    "    # print('参数的数值: ', coefs_1)\n",
    "    coefs_1 = np.array(coefs_1)\n",
    "    for i in range(p):\n",
    "        plt.plot(rang1, coefs_1[:,i], label = 'X%d'%(i+1))\n",
    "    plt.legend(loc = 'best')\n",
    "\n",
    "# 随着 K 值的增加, 岭参数 k 取值越精细, 因为在 [0,1) 范围内的分割更细\n",
    "interact(RR1,K=(1,range_const1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3fac85a1ed43c098a1c44340174ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='K', min=1), Output()), _dom_classes=('widget-interact',)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.RR2(K=1)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 调包: linear_model \n",
    "def RR2(K = 1): \n",
    "    # 初始化一个Ridge Regression\n",
    "    clf = linear_model.Ridge(fit_intercept=False)\n",
    "\n",
    "    # 训练模型: 测试不同的 k 取值，获得系数\n",
    "    rang2 = []\n",
    "    for i in range(K):\n",
    "        rang2.append(i/K)\n",
    "    coefs_2 = []\n",
    "    for k in rang2:\n",
    "        clf.set_params(alpha=k)\n",
    "        clf.fit(X_std1, Y_std)\n",
    "        coefs_2.append(clf.coef_)\n",
    "\n",
    "    # 画图\n",
    "    # print('参数的数值: ', coefs_2)\n",
    "    coefs_2 = np.array(coefs_2)\n",
    "    for i in range(p):\n",
    "        plt.plot(rang2, coefs_2[:,i], label = 'X%d'%(i+1))   \n",
    "    plt.legend(loc = 'best')\n",
    "\n",
    "interact(RR2,K=(1,range_const1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e543c0dbe980495e966cfbcf9d6cec8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='K', max=30, min=1), Output()), _dom_classes=('widget-int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.RR3(K=1)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 岭参数 k 的最大取值范围: [exp(-10),exp(19)]\n",
    "range_const2 = 30\n",
    "\n",
    "def RR3(K = 1):\n",
    "    # 初始化一个Ridge Regression\n",
    "    clf = linear_model.Ridge(fit_intercept=False)\n",
    "    \n",
    "    # 训练模型: 测试不同的 k 取值，获得系数\n",
    "    coefs_3 = []\n",
    "    num_lambda3 = K\n",
    "    for k in range(num_lambda3):\n",
    "        clf.set_params(alpha=np.exp(k-10))  # 岭参数 k 取值范围: [exp(-10),exp((K-1)-10)]\n",
    "        clf.fit(X_std1, Y_std)\n",
    "        coefs_3.append(clf.coef_)\n",
    "\n",
    "    # 画图\n",
    "    # print('参数的数值：', coefs_3)\n",
    "    x3 = range(num_lambda3)\n",
    "    coefs_3 = np.array(coefs_3)\n",
    "    for i in range(p):\n",
    "        plt.plot(x3, coefs_3[:,i], label = 'X%d'%(i+1))\n",
    "        plt.text(x3[-1], coefs_3[-1,i], '%.4f' % float(coefs_3[-1,i]), fontsize=8)\n",
    "    print('岭参数为: ', np.exp(K-10))\n",
    "    print('对应的岭估计: ', coefs_3[-1,:])\n",
    "    plt.legend(loc = 'best')\n",
    "\n",
    "interact(RR3,K=(1,range_const2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于交叉验证的岭回归 alpha 选择可以直接获得一个相对不错的 alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k 的数值 :  0.1\n",
      "参数的数值： [0.39521196 0.71654002 0.30808004 0.15082594]\n"
     ]
    }
   ],
   "source": [
    "# 初始化一个Ridge Cross-Validation Regression\n",
    "clf_cv = linear_model.RidgeCV(fit_intercept=False)\n",
    " \n",
    "# 训练模型\n",
    "clf_cv.fit(X_std1, Y_std)\n",
    "\n",
    "k_cv = clf_cv.alpha_\n",
    "coef_cv = clf_cv.coef_ \n",
    "print('k 的数值 : ', clf_cv.alpha_)\n",
    "print('参数的数值：', clf_cv.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39521196 0.71654002 0.30808004 0.15082594]\n",
      "[0.39521196 0.71654002 0.30808004 0.15082594]\n",
      "差异:  [-6.10622664e-16 -2.22044605e-16 -3.99680289e-15 -3.58046925e-15]\n"
     ]
    }
   ],
   "source": [
    "# 验证岭估计和最小二乘估计之间的关系\n",
    "thres_k = int(k_cv * range_const1)\n",
    "print(coef_cv)\n",
    "\n",
    "C1 = X_std1.T @ X_std1\n",
    "C2 = np.linalg.inv(C1 + k_cv * np.eye(p))\n",
    "C3 = C2 @ C1\n",
    "print(C3 @ beta_std_hat1)\n",
    "\n",
    "diff = coef_cv - C3 @ beta_std_hat1\n",
    "print('差异: ', diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 方差扩大因子法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "岭参数为:  0.049787068367863944\n",
      "对应的岭估计:  [0.42182275 0.75320769 0.35537606 0.11051347]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "coefs_4 = []\n",
    "thres_vif = 5\n",
    "num_lambda4 = 30\n",
    "judge = True\n",
    "for k in range(num_lambda4):\n",
    "    cnt = 0\n",
    "    C = np.linalg.inv(C1 + np.exp(k-10) * np.eye(p)) @ C1 @np.linalg.inv(C1 + np.exp(k-10) * np.eye(p))\n",
    "    # print(k, C)\n",
    "    temp4 = np.linalg.inv(X_std1.T @ X_std1 + np.exp(k-10) * np.eye(p)) @ X_std1.T @ Y_std\n",
    "    coefs_4.append(temp4)\n",
    "    \n",
    "    # 给定 k, 使得所有方差扩大因子 C[j][j] <= thres_vif\n",
    "    for j in range(p):\n",
    "        if C[j][j] < thres_vif:\n",
    "            cnt += 1\n",
    "        else:\n",
    "            break\n",
    "    if cnt == p:\n",
    "        k4 = np.exp(k-10)\n",
    "        print('岭参数为: ', np.exp(k-10))\n",
    "        print('对应的岭估计: ', temp4)\n",
    "        break\n",
    "\n",
    "# 画图\n",
    "coef_4 = temp4\n",
    "# print('参数的数值：', coefs_4)\n",
    "x4 = range(k+1)  # 以 k 作为横坐标\n",
    "# x4 = []  # 以 np.exp(k) 作为横坐标\n",
    "# for i in range(k+1):\n",
    "#      x4.append(np.exp(i-10))\n",
    "coefs_4 = np.array(coefs_4)\n",
    "for i in range(p):\n",
    "    plt.plot(x4, coefs_4[:,i], label = 'X%d'%(i+1))\n",
    "    plt.text(x4[-1], coefs_4[-1,i], '%.4f' % float(coefs_4[-1,i]), fontsize=8)\n",
    "\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 霍尔-肯纳德（Hoerl-Kennad）公式  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "岭参数 k_HK:  0.0026754711436488164\n",
      "对应的岭估计:  [ 0.4512857   0.76968807  0.68446324 -0.21229962]\n"
     ]
    }
   ],
   "source": [
    "SSE = sum((Y_std - Y_std_hat) ** 2)\n",
    "sigma2 = SSE / (n - p - 1)\n",
    "\n",
    "Z = X_std1 @ V.T\n",
    "alpha_hat = np.linalg.inv(W_diag) @ Z.T @ Y_std\n",
    "# print(alpha_hat)\n",
    "\n",
    "k_HK = sigma2 / max(alpha_hat**2)\n",
    "k5 = k_HK\n",
    "print('岭参数 k_HK: ', k5)\n",
    "coef_5 = np.linalg.inv(X_std1.T @ X_std1 + k5 * np.eye(p)) @ X_std1.T @ Y_std\n",
    "print('对应的岭估计: ', coef_5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Mcdorard-Garaneau 公式  \n",
    "如果 $Q=||\\hat{\\beta}||^2-\\hat{\\sigma}^2\\sum_{j=1}^p\\lambda_j^{-1} \\leq 0$，则认为 $\\hat{\\beta}$ 的各个分量都差不多，此时，对 $\\hat{\\beta}$ 不进行压缩，选择 $k = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  1.337631650714247\n",
      "岭参数为:  4.5399929762484854e-05\n",
      "对应的岭估计:  [ 0.45334777  0.76428877  0.78521579 -0.3127196 ]\n"
     ]
    }
   ],
   "source": [
    "thres_diff = 0.2\n",
    "beta_compress = beta_std_hat1.T @ beta_std_hat1 - sigma2 * sum(1/W)\n",
    "if beta_compress <= 0:\n",
    "    k6 = 0\n",
    "    print('k = 0, 不对最小二乘估计进行压缩.')\n",
    "else:\n",
    "    print('Q: ', beta_compress)\n",
    "    coefs_6 = []\n",
    "    num_lambda6 = 30\n",
    "    for k in range(num_lambda6):\n",
    "        temp6 = np.linalg.inv(X_std1.T @ X_std1 + np.exp(k-10) * np.eye(p)) @ X_std1.T @ Y_std\n",
    "        beta_k_compress = temp6.T @ temp6\n",
    "        if abs(beta_compress-beta_k_compress) < thres_diff:\n",
    "            k6 = np.exp(k-10)\n",
    "            print('岭参数为: ', k6)\n",
    "            print('对应的岭估计: ', temp6)\n",
    "            break\n",
    "coef_6 = temp6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3:**\n",
    "\n",
    "岭回归估计虽然在k<>0时是有偏估计，但它的MSE要小于最小二乘估计。尤其在数据具有多重共线性时，选择合适的岭参数可以让估计结果更接近原始的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "岭参数 k =  0.1\n",
      "原始的最小二乘估计 =  [ 2.98037623  3.98795702 11.75164845 -2.2088513 ]\n",
      "标准化后的最小二乘估计 =  [ 0.45338505  0.76416227  0.78738122 -0.31487934]\n",
      "\n",
      "\n",
      "原始 beta =  [3 4 5 1]\n",
      "岭估计 =  [0.39521196 0.71654002 0.30808004 0.15082594]\n",
      "还原岭估计 =  [2.59796903 3.73942931 4.59808825 1.05803093]\n"
     ]
    }
   ],
   "source": [
    "beta_rr = coef_cv\n",
    "print('岭参数 k = ', k_cv)\n",
    "print('原始的最小二乘估计 = ', beta_hat[1:p+1])\n",
    "print('标准化后的最小二乘估计 = ', beta_std_hat1)\n",
    "print('\\n')\n",
    "print('原始 beta = ', beta[1:p+1])\n",
    "print('岭估计 = ', beta_rr)\n",
    "print('还原岭估计 = ', beta_rr *  np.sqrt(Y_L) / np.sqrt(X_L))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由此可知, 岭参数取值为 0.1 左右时，还原后的岭估计接近原始（上帝视角下）的 $\\beta$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
